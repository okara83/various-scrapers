{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/fake_useragent/utils.py\", line 154, in load\n",
      "    for item in get_browsers(verify_ssl=verify_ssl):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
      "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the total number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_pages(url):\n",
    "    useragent = ua.random    # randomly choose a user-agent\n",
    "    headers = {'User-Agent':useragent}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(response.content,'lxml')\n",
    "    pages = soup.find('div',class_=\"pagination\").find_all('li')[-1].find('a').get_text()\n",
    "    pages = int(pages)    # 'pages' is the total number of pages\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to parse each item's page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_retrieve(dic,key):\n",
    "    try:\n",
    "        return dic[key]\n",
    "    except KeyError:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_item(url):\n",
    "    item_url = 'https://www.winemag.com/buying-guide' + url\n",
    "    useragent = ua.random    # randomly choose a user-agent\n",
    "    headers = {'User-Agent':useragent}\n",
    "    response = requests.get(item_url,headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content,'lxml')\n",
    "        try:\n",
    "            description = soup.find('p',class_='description').get_text()\n",
    "        except AttributeError:\n",
    "            description = np.NaN\n",
    "        try:\n",
    "            taster = soup.find('span',class_='taster-area').get_text()\n",
    "        except AttributeError:\n",
    "            taster = np.NaN\n",
    "\n",
    "        info_list = soup.find_all('li',class_='row')\n",
    "        info_dict = dict()\n",
    "\n",
    "        for element in info_list:\n",
    "            tag = element.find_all('div')[0].get_text().strip()\n",
    "            value = element.find_all('div')[1].get_text().strip()\n",
    "            info_dict[tag] = value\n",
    "\n",
    "        designation = help_retrieve(info_dict,\"Designation\")\n",
    "        variety = help_retrieve(info_dict,\"Variety\")\n",
    "        appellation = help_retrieve(info_dict,\"Appellation\")\n",
    "        winery = help_retrieve(info_dict,\"Winery\")\n",
    "        alcohol = help_retrieve(info_dict,\"Alcohol\")\n",
    "        bottle_size = help_retrieve(info_dict,\"Bottle Size\")\n",
    "        category = help_retrieve(info_dict,\"Category\")\n",
    "        importer = help_retrieve(info_dict,\"Importer\")\n",
    "        date_published = help_retrieve(info_dict,\"Date Published\")\n",
    "        user_avg_rating = help_retrieve(info_dict,\"User Avg Rating\")\n",
    "\n",
    "        if user_avg_rating == 'Not rated yet [Add Your Review]':\n",
    "            user_avg_rating = np.NaN\n",
    "\n",
    "        related_items = soup.find_all('li',class_='review-item')\n",
    "        related_items = [element.find('a').get('data-review-id') for element in related_items]    # related_items holds a list of related items' ids\n",
    "\n",
    "        return (description,taster,designation,variety,appellation,winery,alcohol,bottle_size,\\\n",
    "                category,importer,date_published,user_avg_rating,related_items)\n",
    "    else:\n",
    "        print(f\"{item_url} request failed! Status code: {response.status_code}. Skip it.\")\n",
    "        raise RuntimeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to parse each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only store the ending of each item's url to save memory, because the former parts are the same for all\n",
    "pattern = re.compile(r'https://www.winemag.com/buying-guide(?P<ending>.*)')\n",
    "\n",
    "# request and parse each page\n",
    "# return a list with items in this page as tuples inside\n",
    "def parse_page(base_url,i):\n",
    "    url = re.sub(r\"page=\\d+\",f\"page={i}\",base_url)  \n",
    "    useragent = ua.random    # randomly choose a user-agent\n",
    "    headers = {'User-Agent':useragent}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content,'lxml')\n",
    "        li = soup.find_all('li',class_=\"review-item\")\n",
    "        page = list()\n",
    "\n",
    "        for element in li:\n",
    "            try:\n",
    "                name = element.find('h3').get_text()\n",
    "                rating = element.find('span',class_=\"rating\").find('strong').get_text()\n",
    "                price = element.find('span',class_=\"price\").get_text()\n",
    "                price = price.replace('$','')\n",
    "            except AttributeError:\n",
    "                print(f\"Some items are missed from page {i}: {url}. Skip it.\")\n",
    "                continue\n",
    "            try:\n",
    "                price = float(price)\n",
    "            except ValueError:\n",
    "                price = np.NaN\n",
    "            item_url = element.find('a',class_=\"review-listing row\").get('href')\n",
    "            item_url = pattern.search(item_url).group('ending')\n",
    "            item_id = element.find('a',class_=\"review-listing row\").get('data-review-id')\n",
    "            try:\n",
    "                details = parse_item(item_url)\n",
    "            except RuntimeError:\n",
    "                details = (np.NaN,)*13\n",
    "            all_info = (item_id,name,rating,price,item_url) + details\n",
    "            page.append(all_info)\n",
    "            \n",
    "        return page\n",
    "    else:\n",
    "        print(f\"Page #{i} request failed! Status code: {response.status_code}. Skip it.\")\n",
    "        raise RuntimeError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape all the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all(url):\n",
    "    result = list()\n",
    "    start = datetime.datetime.now()\n",
    "    pages = total_pages(url)\n",
    "\n",
    "    for i in range(1,pages+1):\n",
    "        try:\n",
    "            result.extend(parse_page(url,i))\n",
    "            time.sleep(random.random())\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        except:\n",
    "            print(r\"working lag\")\n",
    "        finally:\n",
    "            if i%10 == 0:\n",
    "                print(f\"Process overview: {i} pages have been scraped...\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    interval = (end - start).total_seconds()\n",
    "    hour = int(interval // 3600)\n",
    "    minute = int((interval % 3600) // 60)\n",
    "    second = int((interval % 3600) % 60)\n",
    "    print(f\"For base url: {url}\")\n",
    "    print(f\"Scraping has been done in {hour}h {minute}min {second}s.\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url_list(urls, file_name=\"raw_data.csv\"):\n",
    "    result = list()\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            print(f\"Begin scraping from base url: {url}\")\n",
    "            result.extend(scrape_all(url))\n",
    "        except:\n",
    "            print(f\"Scraping from base url: {url} FAILED! Going to the next one...\")\n",
    "            continue\n",
    "        \n",
    "    df = pd.DataFrame(result,columns=['id','name','rating','price','item_url','description','taster',\\\n",
    "                                      'designation','variety','appellation','winery','alcohol','bottle_size',\\\n",
    "                                        'category','importer','date_published','user_avg_rating','related_items'])\n",
    "    df.to_csv(file_name,encoding='utf-8')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.winemag.com/?s=&rating=98.0-*&drink_type=wine&page=1&search_type=reviews',\\\n",
    "       'https://www.winemag.com/?s=&rating=94.0-97.99&drink_type=wine&page=1&search_type=reviews',\\\n",
    "       'https://www.winemag.com/?s=&rating=80.0-82.99&drink_type=wine&page=1&search_type=reviews',\\\n",
    "       'https://www.winemag.com/?s=&rating=90.0-93.99&price=16.0-25.99,26.0-40.99&drink_type=wine&page=1&search_type=reviews',\\\n",
    "        'https://www.winemag.com/?s=&rating=83.0-86.99&price=41.0-60.99,61.0-75.99,76.0-99.99,100.0-199.99,200.0-*&drink_type=wine&page=1&search_type=reviews',\\\n",
    "       'https://www.winemag.com/?s=&rating=90.0-93.99&price=1.0-15.99,41.0-60.99,61.0-75.99,76.0-99.99,100.0-199.99,200.0-*&drink_type=wine&page=1&search_type=reviews',\\\n",
    "        'https://www.winemag.com/?s=&rating=83.0-86.99&price=1.0-15.99&drink_type=wine&page=1&search_type=reviews',\\\n",
    "       'https://www.winemag.com/?s=&rating=87.0-89.99&price=1.0-15.99,26.0-40.99,41.0-60.99&drink_type=wine&page=1&search_type=reviews',\\\n",
    "        'https://www.winemag.com/?s=&rating=83.0-86.99&price=26.0-40.99&drink_type=wine&page=1&search_type=reviews',\\\n",
    "       'https://www.winemag.com/?s=&rating=87.0-89.99&price=16.0-25.99,61.0-75.99,76.0-99.99,100.0-199.99,200.0-*&drink_type=wine&page=1&search_type=reviews',\\\n",
    "       'https://www.winemag.com/?s=&rating=83.0-86.99&price=16.0-25.99&drink_type=wine&page=1&search_type=reviews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin scraping from base url: https://www.winemag.com/?s=&rating=98.0-*&drink_type=wine&page=1&search_type=reviews\n",
      "working lag\n"
     ]
    }
   ],
   "source": [
    "df = scrape_url_list(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
